version: '3.8'

services:
  ollama:
    image: docker.io/ollama/ollama:latest
    container_name: ollama
    networks:
      - matrixpod
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: 1
#              capabilities: [ gpu ]
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ./data/ollama:/root/.ollama
    privileged: true


  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: open-webui
    restart: always
    ports:
      - "127.0.0.1:3002:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - ./data/open-webui:/app/backend/data
    networks:
      matrixpod:
        aliases:
          - ollama.local
    depends_on:
      - ollama
    restart: unless-stopped

networks:
  matrixpod:
    external: true
    name: matrixpod

volumes:
  open-webui: {}
